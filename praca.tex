% !TEX encoding = UTF-8 Unicode
%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[licencjacka]{pracamgr}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{amsfonts}
\lstset{
	basicstyle=\small\ttfamily,
	columns=flexible,
	breaklines=true
}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Dane magistranta:
\autor{Marcin Basiuk}{mbp-18217}

\title{Przegląd wybranych algorytmów uczenia maszynowego i sztucznej inteligencji na przykładzie gry w kółko i krzyżyk}


%\tytulang{Przegląd wybranych algorytmów uczenia maszynowego
%oraz sztucznej inteligencji na przykładzie gry w kółko i krzyżyk}

%kierunek:
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Data Science w zastosowaniach biznesowych.
Warsztaty z wykorzystaniem programu R.}

\opiekun{dr. Piotr Wójcik\\
  Zakład Finansów Ilościowych\\
 }

% miesiąc i~rok:
\date{Czerwiec 2018}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{
%D. Software\\
%  D.127. Blabalgorithms\\
%  D.127.6. Numerical blabalysis
}

% Słowa kluczowe:
\keywords{mimimax,teoria gier, uczenie maszynowe, sieci neuronowe, reinformcent learning }

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}

\maketitle
%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  W pracy zaimplementowano trzy różne algorytmy uczące komputer gry w kółko i krzyżyk. Pierwszym algorytmem jest Minimax połączony z przycinaniem $\alpha$ - $\beta$. Następnie wykorzystano algorytm sieci neuronowych oraz algorytm genetyczny. Praca zawiera, opis porównanie działania algorytmów oraz aplikację do gry z komputerem wykorzystującym wybrany algorytm.
\end{abstract}

\tableofcontents

%\listoffigures
%\listoftables

\chapter*{Wprowadzenie}
\addcontentsline{toc}{chapter}{Wprowadzenie}

W trakcie zajęć na studiach podyplomowych wiele uwagi poświęcone było budowaniu modeli predykcyjnych. Pracując z  programem R  i RStudio tworzyliśmy modele i szacowaliśmy parametry tych modeli, tak żeby ,,na końcu'' mieć narzędzie, albo posługując się programistyczną nomenklaturą funkcję, która przyjmuje dane wejściowe i zwraca predykcję na podstawie tych danych. Wykorzystywaliśmy w tym celu szereg metod, zaczynając od regresji liniowej lub regresji logistycznej jeżeli zmienna objaśniana miała charakter nominalny, drzewa decyzyjne,  algorytmy k - najbliższych sąsiadów, analizę głównych składowych, czy w końcu sieci neuronowe.  \\

Doświadczenie z zajęć jednoznacznie wskazywało na dwa kluczowe aspekty, które decydowały o skuteczności modeli predykcyjnych. Po pierwsze jakość danych. Zaszumienie danych, bądź niepoprawne skalowanie prowadziło najczęściej do wyników poniżej oczekiwań. Druga sprawa dotyczyła tego na ile zmienne objaśniające rzeczywiście determinują zmienną (lub zmienne) objaśniane. Intuicyjnie, mało kto uwierzy, że dane ankietowe dotyczące średniej ilości godzin spędzonych w pracy połączonych z danymi demograficznymi takimi jak wiek, poziom wykształcenia, miejsce zamieszkania itd. będzie determinować poziom przychodu. Sukces finansowy najprawdopodobniej zależy również od szeregu cech osobowości takich jak odwaga do podążania za swoimi celami, umiejętność budowania swojej pozycji w systemach społecznych itp.  Modele bazujące na danych ankietowych i demograficznych - takie jak budowaliśmy na zajęciach - miały skuteczność mierzoną średnim odchyleniem kwadratowy rzędy $70\% - 85\%$  Trudno jednoznacznie powiedzieć na ile satysfakcjonujące są takie wyniki. Można powiedzieć, że model ,,działa'', można też w ten sposób porównywać modele, ale trudno powiedzieć na ile ,,uchwycono'',  prawa natury rządzące poziomem przychodu.\\
 
 Przymierzając się do napisania tej pracy chciałem skoncentrować się bardziej na algorytmach uczących komputery, aniżeli na zbieraniu, czyszczeniu i doborze danych, czyli na tzw. ,,feature engineering'''.  Chcę podkreślić że w żadnym wypadku nie pomniejszam wagi ani znaczenia tej sztuki. Po prostu bardziej interesuje mnie sam proces uczenia się komputerów,  aniżeli proces dostarczania odpowiednich danych, żeby to uczenie było skuteczne. Szukałem więc problemów, w których dane byłyby niejako naturalnie dostępne i tak moja uwaga skupiła się na grach. Jak nauczyć komputer grać w. prostą grę?  W tym wypadku dane są zbierane poprzez grę komputera z człowiekiem, z graczem losowym lub z samym sobą (komputer vs. komputer) i oczywiście nie ma mowy o żadnym zaszumieniu. Szachy i warcaby wydawały się zbyt ambitnym wyzwaniem. Rozważałem gry karciane, ale wolałem skupić się na grze w pełni deterministycznej, w której losowość nie miałaby wpływu na wynik. Dzięki temu wiadomo byłoby na ile komputer dobrze nauczył się grać, bez względu na to jak dobra trafiła mu się karta. Ostatecznie wybór padł na kółko i krzyżyk - prosta i w pełni deterministyczna gra. Dla mnie idealne pole do eksperymentowania z uczeniem komputera. \\

W pierwszym rozdziale opisuję zaimplementowany algorytm minimax - jest to klasyczne podejście stosowane do uczenia komputerów grania w gry turowe, gdzie jest dwóch graczy i jeżeli jeden wygrywa, to drugi przegrywa; ewentualnie może być remis.  Można polemizować na ile minimax wpisuje się w uczenie maszynowe. Jednak skoro uczy się takiego podejścia na przedmiocie "Artificial Inteligence"  na MIT \cite{MIT_AI},  to chciałem, żeby pojawiło się t tej pracy.\\

W kolejnych dwóch rozdziałach prezentuję algorytmy typowe dla ,,reinforcement learning'' (RL). Kiedy komputer uczy się grać w kółko i krzyżyk, to po wykonaniu ruchu, o ile ruch nie kończy gry,  nie ma natychmiastowej informacji zwrotnej, czy to był dobry ruch, czy też zły ruch. W tym sensie, uczenia komputera gry w kółko i krzyżyk (oraz w inne gry) nie wpisuje się w schemat uczenia z nadzorem. Nie wpisuje się również w uczenie bez nadzoru, bo w przypadku gry, możemy na końcu powiedzieć kto wygrał, a kto przegrał. Z pomocą przychodzi RL, o którym Richard S. Sutton i Andrew Barto w swojej książce \cite{RL} piszą, że jest innym paradygmatem uczenia maszynowego, wychodzącym poza uczenie z nadzorem i bez nadzoru.  W rozdziale drugim komputer ucząc się grać w kółko i krzyżyk będzie szacował prawdopodobieństwo wygrania z danego stanu gry i będzie te wartości zapisywał te dane w pliku tekstowym, do którego również odwołuje w kolejnych grach. Takie podejście staje się niemożliwe w przypadku gier z bardzo dużą przestrzenią stanów (np. szachy, czy GO). Plik byłby zbyt duży i przeszukiwanie pliku zajmowałoby za dużo czasu. W takim wypadku można zaimplementować sieć neuronową, które będzie szacować prawdopodobieństwa wygrania. To ostatnie podejście opisane zostało w rozdziale trzecim. \\

Ponieważ koncentruje się na algorytmach, i implementacja tych algorytmów jest również częścią pracy, to chciałem wykorzystać uniwersalny obiektowy język programowania. Zdecydowałem się na Python 2.7, który w moim odczuciu łatwiej umożliwia tworzenie i obsługę klas obiektów od R.  Do pracy dołączam:
\begin{enumerate}
	\item \textbf{DODATEK A} - Kod z implementacją opisanych algorytmów.
	\item \textbf{DODATEK B}  - aplikacja do gry w kółko i krzyżyk
\end{enumerate}


\chapter{Minimax}\label{r:Minimax}


\section{Opis algorytmu}

W takich grach jak kółko i krzyżyk, szachy lub warcaby możemy mówić o przestrzeni stanów gry. Każdy stan  gry jest reprezentowany przez położenie pionków albo rozmieszczenie kółek i krzyżyków na kwadratowej planszy z 9 polami. Za każdym ruchem, jednego z dwóch graczy, gra przechodzi do innego stanu. Wszystkie możliwe gry są reprezentowane przez drzewo (drzewo w sensie teorii grafów), którego korzeniem jest początkowy stan gry, a kolejne pokolenia, reprezentują na przemian, możliwe ruchy graczy.  Każda ścieżka od korzenia do wybranego liścia  odpowiada jednej w pełni rozegranej grze. I na odwrót, każda możliwa realizacja gry ma odpowiadającą jej ścieżkę na drzewie gry.  Koncepcja ta  jest zilustrowana na poniższym rysunku zaczerpniętym z książki \cite{RL} \\
\begin{center}
	\includegraphics [scale=0.60] {game_tree.png}
\end{center}

Przypuśćmy dalej, że z każdym stanem końcowym gry (liściem) może powiązać liczbę wygranych punktów. W przypadku gier takich jak kółko i krzyżyk, może to być jedna z trzech wartości ${-1, 0, 1}$  odpowiadająca przegranej, remisowi i wygranej odpowiednio.  Możemy jednak wyjść poza ten schemat i rozważać dla stanów końcowych wiele innych wartości.  Ważne jest to, że celem gracza jest maksymalizacja zdobytych punktów. Warto jeszcze zaznaczyć, że punkty przypisane do liście są różne dla rywalizujących ze sobą graczy. W naszym przypadku gry w kółko i krzyżyk, suma punktów zdobytych przez jednego rywala, będzie równa sumie punktów zdobytych przez drugiego rywala, ale z przeciwnym znakiem.  W ogólnym. przypadku jednak tak nie musi być. \\

 Można by naiwnie przypuszczać, że najlepszą strategią będzie wybranie takiego ruchu, który ,,otworzy'' nam ścieżkę do liści z największą możliwą ilością punktów. Zobaczymy na poniższym przykładzie wziętym ze strony \cite{Game}, że taka strategia nie jest optymalna. Wyobraźmy sobie dwie konkurujące ze sobą firmy, które rozważają poniesienie wydatków na promocję oferowanego przez siebie produktu. Firma 1 pierwsza będzie podejmować decyzję, czy inwestować w reklamę, czy też nie.  Ta decyzja odpowiada pierwszemu rozgałęzieniu na poniższym drzewie. Kolejna decyzja  należy do firmy drugiej - inwestować w reklamę lub nie. W sumie mamy cztery możliwe sytuacje (cztery różne rozgrywki) i odpowiadające im punkty (można je interpretować np. jako zysk). \\

\begin{center}
	\includegraphics [scale=0.30] {advertise.png}
\end{center}
 
 Jeżeli obie firmy nie zdecydują się nie inwestować w reklamę, to firma 1 zdobędzie 16 punktów, a firma 2 - 12 punktów. Przyjmijmy, że koszt reklamy to 8 punktów. Jeżeli jedna firma zdecyduje się na reklamę, a druga nie, to firma reklamująca uzyska 13 punktów, a firma która nie reklamowała swojego produktu 7 punktów (firma reklamująca się zdobywa 21 punktów z 28 dostępnych, ale ponosi 8 punktowy koszt reklamy).  Natomiast jeżeli obie firmy zdecydują się na reklamę, to podzielą między siebie rynek po równo, ale ponieważ każda z nich poniesie koszt 8 punktów, to finalnie każda zarobi po 6 punktów ($(28 - 16)/2$).\\
 
 Załóżmy, że jesteśmy prezesem firmy 1 i chcemy podjąć decyzję o tym, czy inwestować w reklamę czy też nie. Załóżmy dalej, że drzewo przedstawiające możliwe wyniki tej gry jest znane wszystkim zainteresowanym stronom. Wówczas logicznym będzie następujące rozumowanie. Jeżeli firma 1 zdecyduje się na reklamę, to firma 2 chcąc maksymalizować liczbę zdobytych punktów nie będzie się reklamować. Wówczas firma 1 zdobędzie 13, a firma 2  - 7 punktów.  Z drugiej strony, jeżeli firma 1 zdecyduje się nie reklamować, to firma 2 znowu maksymalizując swoją wygraną zainwestuje w reklamę i końcowy wynik będzie 7 i 13 punktów dla firmy 1 i firmy 2 odpowiednio.  Ponieważ pierwszy scenariusz jest korzystniejszy, to firma 1 decyduje się na inwestycję w reklamę. \\
 
 Z powyższego, bardzo prostego rozumowania, wynika kilka wniosków. Po pierwsze, przy założeniu, że oponent (firma 2 w tym przypadku) racjonalnie usiłuję maksymalizować liczbę zdobytych punktów, to wybór ,,otwierający'' drogę do liści z największą liczbą punktów nie musi być wyborem optymalnym. Firma 1 mogłaby zdecydować nie reklamować się, bo wówczas jej potencjalna wygrana, to 7 lub 16 punktów versus 6 i 13 przy reklamowaniu się. Taka strategia byłaby słuszna gdyby firma 2 losowo podejmowała decyzje, ale nie jeżeli firma  2 ma racjonalnych decydentów. Druga obserwacja, często podnoszona w teorii gier, że najlepsze rozwiązanie dla obydwu firm - nie reklamować swoich produktów  - jest rozwiązaniem niemożliwym lub przynajmniej nie stablinym. Teoretycznie prezesi firm mogli się zmówić, że nie będą inwestować w reklamę.  Ale w takiej sytuacji, o ile firma 1 rzeczywiście zdecyduje nie reklamować się, to firma 2, chcąc maksymalizować swoje punkty, powinna zdecydować się na reklamę i tym samym złamać umowę. Stąd wspomniana wyżej niestabilność.\\
 
Na powyższym przykładzie można też zrozumieć w jaki sposób wybrać optymalny ruch. Załóżmy, że reprezentujmy firmę 1. Do nas należy otwierający ruch. Racjonalnie będzie założyć, że po naszym ruchu firma 2 wykona ruch, który pozwoli w ostatecznym rachunku na maksymalizację jej wygranej, co przy stałej puli możliwych do wygrania punktów, będzie tożsame z takim ruchem, który zminimalizuje naszą wygraną. Zatem należy wybrać taki ruch, który maksymalizuje minimum z naszych wygranych do jakich może doprowadzić firma 2 kolejnym ruchem.  To rozumowanie naturalnie przekłada się na gry, których drzewa mają więcej pokoleń. W każdym przypadku konieczne jest przejście wszystkich węzłów drzewa idąc od liśc (tutaj wiem jakie są wygrane) biorąc pod uwagę dla każdego pokolenia czy jest nasz ruch, czyli ruch, czy też ruch naszego oponenta, który ma na celu minimalizowanie naszej wygranej. Przedstawiając powyższe rozumowanie w punktach dochodzimy do algorytmu minimax, który dla każdego węzła zwraca wygraną wartość gracza . \\

\begin{enumerate}
	\item{Jeżeli to jest ruch końcowy, to zwróć wartość wygranej i skończ}
	\item{Jeżeli ruch należy do nas, a nie do oponenta to:}
	\begin{enumerate}
		\item{ustawa wartość$=-\infty$}
		\item{Dla każdego możliwego dostępnego ruchu: }
		\item{wartość = max(wartość,  wartość algorytmu minimax na poddrzewie, którego korzeniem jest rozpatrywany ruch oponenta}
		\item{zwróć wartość}
	\end{enumerate}
		\item{Jeżeli ruch należy do oponenta to:}
	\begin{enumerate}
		\item{ustawa wartość$=+\infty$}
		\item{Dla każdego możliwego dostępnego ruchu: }
		\item{wartość = min(wartość,  wartość algorytmu minimax na poddrzewie, którego korzeniem jest rozpatrywany ruch dla nas}
		\item{zwróć wartość}
	\end{enumerate}
\end{enumerate}


Okazuje się, że jeżeli gracze stosują się do powyższego algorytmu, to znaczy wybierają ruchy, które zapewnią im największą wygraną wyliczoną według powyższego algorytmu, to takie strategie będą w równowadze Nash'a. Mówiąc obrazowo są  to najlepsza strategie, w tym sensie, że żadnemu z graczy nie opłaca się ich zmieniać. Jeżeli wiem, że mój oponent będzie grał zgodnie z algorytmem minimax, to moją najlepszą strategią będzie robić to samo. Jeżeli mój oponent będzie grał inną strategią, to być może istnieje lepsza odpowiedź na jego strategie niż minimax.  Jeżeli ja gram według algorytmu minimax, a mój oponent nie, to gra strategią suboptymalną. 

\section{Przycinanie $\alpha$ - $\beta$}
Podstawowym problemem w zastosowaniach algorytmu minimax jest ilość niezbędnych operacji przy dużych drzewach reprezentujących gry.  Dla gry w kółko i krzyżyk, górnym ograniczeniem ilości możliwych gier (liści na drzewie gry) jest $9! = 362880$. W rzeczywistości ilość możliwych partii ogranicza się do kilku tysięcy, ale i tak przeliczenie wszystkich węzłów takiego drzewa na współczesnym komputerze średniej klasy zajmuje kilka sekund (przynajmniej przy amatorskiej implementacji autora niniejszej pracy). Jeżeli pomyśleć o grach takich jak szachy, w których ilość możliwych ruchów i łączna ilość ruchów w grze jest ogromna, to jasnym się staje, że algorytm minimax ma poważne ograniczenia.  \\

W praktyce, żeby ograniczyć ilość wykonywanych operacji stosuje się tzw. przycinanie  $\alpha$ - $\beta$. Polega to na tym, że jeżeli wiemy, że sprawdzenie i porównanie wartości dla kolejnego ruch/węzła na drzewie gry nie zmieni wyboru gracza , to pomijamy takie obliczenie. Najlepiej zobaczyć to na prostym przykładzie. 
\begin{center}
	\includegraphics [scale=0.3] {advertise2.png}
\end{center}
Spójrzmy raz jeszcze na poprzedni przykład, ale nieco zmieniony. Jesteśmy graczem rozpoczynającym grę i chcemy wybrać optymalny ruch za pomocą algorytmu minimax. Minimax jest algorytmem rekurencyjnym; żeby policzyć wartości wygranej dla każdego z możliwych ruchów, trzeba "wywołać" minimax dla gracza minimalizującego na odpowiednich poddrzewach. To z kolei sprowadza się do porównywania wszystkich wartości liści, czyli ostatecznej wygranej w grze i następnie sukcesywnego podążania w górę drzewa.  Idąc od lewego dolnego liścia widzimy, że gracz minimalizujący będzie miał wygraną 6, bo  na pewno nie wybierze liścia z wartością 13 (minimalizuje naszą wygraną). Czyli jeżeli wybierzemy ruch "w lewo", to nasza wygrana, to będzie 6. Jeżeli zaczniemy kalkulować wygraną w przypadku ruch "w prawo", to widzimy, że nasz oponent wybierze 5 albo jeszcze mniej gdyby w skrajnym prawym liściu była mniejsza wartość. Ale ten fakt jest dla nas, dla gracza maksymalizującego, już bez żadnego znaczenia. Decydując się na ruch "w lewo" wygramy 6, a decydując się na ruch "w prawo" wygramy co najwyżej 5. Możemy zatem w ogóle nie sprawdzać jaka jest wartość prawego skrajnego liścia i nie wyliczać wartości wygranej gdybyśmy wykonali ruch "w prawo". Tym sposobem zaoszczędziliśmy trochę obliczeń. 
\begin{flushleft}
	\includegraphics [scale=0.45] {big_tree2.png}
\end{flushleft}
Załóżmy, że mamy do czynienia z takim drzewem gry jak na rysunku. Ostatni wiersz przedstawia możliwe wygrane gracza maksymalizującego, rozpoczynającego grę. Na rysunku zaznaczono węzły i całe podrzewa, których gracz maksymalizujący nie potrzebuje rozpatrywać. Po lewej stronie węzłów zapisana jest maksymalna możliwa wygrana gracza maksymalizującego o ile gra znajdzie się w tym weźle (czyli wartość jaką zwraca algorytm minimax). Pogrubioną linią widać optymalną ścieżkę gry, która kończy się zdobyciem 8 punktów. Dla zwięzłości pracy nie opisuję całego toku rozumowania. Przykład ten wzięty jest z 6 wykładu profesora Winstona z \cite{MIT_AI}.\\

Jak widać, w niektórych przypadkach udaje się sporo zaoszczędzić w ilości obliczeń.  Warto jeszcze wyjaśnić skąd $\alpha - \beta$ w nazwie. Zaczynając przeszukiwanie drzewa ustawiamy parametry $\alpha$ i $\beta$ na $+\infty$ i $-\infty$ odpowiednio, i interpretujemy je jako najgorszy możliwy wynik gracza minimalizującego ($+\infty$) i  najgorszy możliwy wynik gracza maksymalizującego ($-\infty$). Przechodząc rekurencyjnie przez drzewo sprawdzamy aktualizujemy wartości $\alpha$ i $\beta$ i jeżeli dla danego poddrzewa  nie możemy poprawić wyniku  $\alpha$ lub  $\beta$ (w zależności czy dla tego poddrzewa pierwszy ruch należy do gracza minimalizującego lub maksymalizującego), to nie przeszukujemy dalej tego poddrzewa.


\section{Funkcja wartości}
Nietrudno wyobrazić sobie, że dla gier z olbrzymimi drzewami gry, przycinanie $\alpha$ - $\beta$ pomoże nam zejść o o kilka pokoleń w dół drzewa, ale i tak nawet najszybszy komputer nie jest w stanie przeszukać całego drzewa w takiej grze jak szachy. W związku z tym,  algorytmy grające w takie gry wykorzystują \textbf{funkcję wartości}. Jest to funkcja zwracająca pewną wartość (scoring) dla każdego stanu gry. Czym wyższa wartość, tym bardziej korzystna sytuacja dla gracza. \\

Algorytm grający będzie z reguły miał zaszyty parametr, który każe przerwać przeszukiwanie drzewa na danym poziomie lub po przekroczeniu określonej ilości czasu. Następnie ze wszystkich możliwych ruchów (liści z przeszukanego poddrzewa do momentu zastopowania algorytmu) wybierany jest ten dla którego, funkcja wartości przyjmuje maksymalną wartość. Dla szachów funkcja wartości może np. zliczać ilość punktów (każda figura ma przypisaną liczbę punktów - pionek 1, koń 5 , wieża 8 itd)  gracza i oponenta figur będących na planszy. 


\section{Implementacja}
Szczegóły dotyczące implemetacji, jak również kod można znaleźć w \textbf{DODATEK B}. W tej sekcji chciałbym ograniczyć się do kilku spostrzeżeń związanych z implementacją algorytmu minimax z przycinaniem $\alpha - \beta$.\\

Liczyłem na to, że zastosowanie przycinania zmniejszy czas wykonania algorytmu na początku gry, w pierwszych dwóch ruchach,  kiedy konieczne jest przeszukanie największych drzew. Od trzeciego ruchu, wykonanie algorytmu jest rzędu $0.1$ sekundy, więc optymalizacje przestają mieć znaczenie.  Niestety zastosowanie przycinania $\alpha - \beta$ nie poprawiło w widoczny sposób szybkości działania algorytmu. Żeby ograniczyć czas oczekiwania na odpowiedź komputera dodałem instrukcję, powodującą, że jeżeli algorytm zaczyna grę, to zawsze wybiera środkowe pole.\\

Nie udało mi się też zaimplementować algorytmu z ograniczeniem głębokości przeszukiwania drzewa i wykorzystującego funkcję wartości. Banalne funkcje wartości typu sprawdzenie,  czy mam dwa krzyżyki(kółka) w linii nie dawały dobrych rezultatów. Tak skalibrowany algorytm popełniał oczywiste błędy i łatwo z nim było wygrać.  Natomiast uwzględnienie większej ilości niuansów, w zasadzie prowadziło mnie  do napisania reguły jak grać w kółko i krzyżyk. Wówczas minimax przestaje być potrzebny i można się ograniczyć do do policzenia funkcji wartości dla możliwych ruchów i wybrania tego z największą wartością. Niestety nie udało mi się znaleźć ,,złotego środka'' i w końcowej implementacji nie korzystam z funkcji wartości. \\

Algorytm minimax jest graczem optymalnym, nie można grać lepiej od algorytmu minimax. W szczególności, można co najwyżej zremisować. W dalszej części pracy algorytm minimax będzie obok gracza losowego (takiego co losuje z jednostajnym prawdopodobieństwem kolejny ruch z puli możliwych) punktem odniesienie skuteczności algorytmu grającego w kółko i krzyżyk. Skuteczne algorytmy powinny często remisować i rzadko przegrywać z minimax, oraz często wygrywać i rzadko remistować z minimax. Poniżej przedstawiam wyniki partii 100 gier rozegranych pomiędzy graczem stosującym minimax oraz graczem losowym  oraz 100 gier rozegranych pomiędzy dwoma graczami stosującymi algorytm minimax.\\ 

\begin{flushleft}
	\includegraphics [scale=0.6] {random_minimax.png}
\end{flushleft}

Gracz stosujący minimax nie przegrał ani razu, za to zremisował z graczem losowym 13 razy na 100 gier.

\begin{flushleft}
	\includegraphics [scale=0.6] {minimax_minimax.png}
\end{flushleft}
Dwóch graczy stosujących minimax zawsze będą ze sobą remisować. 

\chapter{Q-learning}\label{r:Tablica}

\section{Procesy decyzyjne Markowa}
W rozdziale pierwszym gry były reprezentowane (a w zasadzie wszystkie możliwe rozgrywki) za pomocą drzew, gdzie węzły należały do możliwych stanów gry, a krawędź pomiędzy dwoma węzłami oznaczała przejście na skutek ruchu jednego z graczy do kolejnego stanu. W tym rozdziale skoncentruję się na innym modelu, a mianowicie na procesach decyzyjnych Markowa.  Przed formalnymi definicjami kilka chciałbym przedstawić heurystyczny opis modelu, posiłkując się rysunkiem wziętym z \cite{RL}.\\

\begin{flushleft}
	\includegraphics [scale=0.6] {agent_env.png}
\end{flushleft}

Załóżmy, że mamy agenta (gracza w naszym przypadku), który jest w interakcji z pewnym systemem (\textit{environment} na rysunku). Agent znajduje się w pewnym stanie $s$, w dyskretnym czasie $t$ i ma do wyboru podjęcie jednej z dostępnych akcji $a\in\mathcal{A}$. Na skutek podjęcia akcji $a$ system odpowie przejściem do nowego stanu $s'$ oraz nagrodą $r$ (z ang. \textit{reward}) w czasie $t+1$.  Nie jest przy tym powiedziane, że dla konkretnej pary $(s,a)$ system zawsze przejdzie do tego samego stanu $s'$. System może wybierać swoją odpowiedź losowo. Podobnie przy przejściu ze stanu $s$ do $s'$ system może losowo zwracać różne nagrody $r$.  Natomiast ta losowość musi zależeć tylko i wyłącznie od pary $(s,a)$ i nie może w żaden sposób zależeć od poprzednich stanów w jakich system się znajdował. Na tym polega istota ,,markowości", że historia nie ma znaczenia. W tym miejscu chciałbym poczynić jeszcze jedno założenie, że zbiór możliwych stanów systemu jest skończony. To założenie nie jest konieczne w ogólnej teorii, ale pozwala na daleko idące uproszczenia, z których skrzętnie korzystam w tej, jakby nie było, ilustratywnej pracy. Interakcja agenta z systemem przez kilka kolejnych kroków począwszy od $t=0$ można przedstawić za pomocą takiej trajektorii:
$$s_{o}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, s_{2}, a_{2}, r_{3}, s_{3},...$$

Formalizując powyższe rozważmy czwórkę postaci $<\mathcal{S},\mathcal{A},\mathcal{R},p>$ gdzie:
\begin{enumerate}
 	\item{$\mathcal{S}$ jest skończonym zbiorem możliwych stanów systemu}
	\item{$\mathcal{A}$ jest zbiorem możliwych akcji. Formalnie $\mathcal{A}$ jest kolekcją zbiorów $\mathcal{A}_{s}$ indeksowaną stanami $s\in\mathcal{S}$. Chodzi o to, że dla różnych stanów systemy dostępne są różne akcje.  Dla uproszczenie będę jednak pomijali ten niuans notacyjny i w dalszej części mówił o możliwych akcjach $\mathcal{A}$  pamiętając, że ten zbiór może być ograniczony konkretnym stanem.}
	\item{$\mathcal{R}\subseteq\mathbf{R}$ jest zbiorem wartości nagród, jakie agent może otrzymać}
	\item{$p:\mathcal{S}\times\mathcal{A}\times\mathcal{R}\times\mathcal{S}\rightarrow [0,1]$ jest funkcją determinującą dynamikę procesu decyzyjnego Markowa. Dla każdej pary stanu i wyboru 		akcji w czasie $t$ funkcja $p$ określa rozkład prawdopodobieństwa na $\mathcal{R}\times\mathcal{S}$ w czasie $t+1$:  $$p(s, a, s', r) = \mathbf{P}(S_{t+1}=s', R_{t+1}=r | S_{t}=s, A_{t}=a )$$}
\end{enumerate}

Opisany model z powodzeniem można stosować do opisu gry w kółko i krzyżyk (jak również innych turowych gier). Przestrzeń stanów $\mathcal{S}$ w tym wypadku to będą wszystkie możliwe stany gry. Akcje $\mathcal{A}$ to będą wszystkie możliwe ruchy do wykonania przy danym stanie gry. Agentem jest gracz - algorytm, którego staram się nauczyć grać w kółko i krzyży. Systemem natomiast jest drugi gracz - może to być gracz losowy lub gracz stosujący algorytm minimax. Pozostaje jeszcze określenie nagród $\mathcal{R}$. Wybór jest poniekąd arbitralny, ale ponieważ celem będzie maksymalizowanie wartości oczekiwanej nagród, to należy pamiętać, że algorytm wyuczy się osiągania średnio wysokich nagród, a nie wygrywanie. \\

Zanim przejdę do opisu algorytmu potrzebne będzie jeszcze kilka pojęć. \textbf{Strategią} nazwiemy dowolne mapowanie 
$$\pi : \mathcal{S}\rightarrow\mathbf{P}_{\mathcal{A}} \text{, gdzie } \forall_{s\in\mathcal{S}} \text{ } \pi(s) \text {  jest rozkładem prawdopodobieństwa na } \mathcal{A}$$ 
Celem gracza będzie dążenie do takiej strategii, która maksymalizuje wartość nagród otrzymanych podczas gry.  W procesach decyzyjnych Markowa często stosuje się dyskontowanie przyszłych nagród wybranym czynnikiem $\gamma\leq1$ Wynika to z dwóch powodów. Po pierwsze w zastosowaniach często przyszłe nagrody mają rzeczywiście niższą wartość niż obecnie (jak w matematyce finansowej) a po drugie, interakcja agenta z system może nie być ograniczona ilością ruchów, a wtedy suma przyszłych nagród niezależnie od wyboru strategii $\pi$ może być nieskończona, co uniemożliwia wybranie optymalnej strategii. Jeżeli natomiast maksymalna nagroda jest ograniczona, powiedźmy liczbą $M$, oraz $\gamma<1$, to suma nagród jest z góry ograniczona przez
$$M+\gamma M +\gamma^{2}M+\gamma^{3}M+...  = \frac{M}{1-\gamma}$$
Dla uproszczenia zapisu wprowadźmy oznaczenie na sumę przyszłych zdyskontowanych nagród:
$$G_{t} \stackrel{\text{def}}{=} R_{t} + \gamma R_{t+1} + \gamma^{2}R_{t+2} +...$$

Wartością stanu $s$ przy strategii $\pi$ jest z definicji 
$$v_{\pi}(s)\stackrel{\text{def}}{=} \mathbb{E}_{\pi}(G_{t}| S_{t} = s) $$
Jest to wartość oczekiwana sumy dyskontowanych przyszłych nagród pod warunkiem, że stosujemy strategię $\pi$ począwszy od czasu $t$. w którym to czasie system znajdował się w stanie $s$. 
Podobnie zdefiniujmy jeszcze funkcję określającą wartość akcji $a$ podjętą w stanie $s$ jako zdyskontowaną wartość przyszłych nagród przy założeniu, że stosujemy strategię $\pi$, a w stanie $s$ wybierzemy akurat akcję $a$ (strategia $\pi$ zadaje rozkład prawdopodobieństwa na $\mathcal{A}$, i w tej definicji zakładamy, że akurat wylosowano akcję $a$):
$$q_{\pi}(a,s) \stackrel{\text{def}}{=} \mathbb{E}_{\pi}(G_{t}| S_{t} = s, a_{t} = a) $$

Co zatem oznacza znalezienie optymalnej strategii? Intuicyjnie jest to oczywiste. Chcemy mieć strategię, która będzie maksymalizować wartość oczekiwaną sumy przyszłych zdyskontowanych nagród. Formalnie $\pi_{1}\prec\pi_{2}$ jeżeli dla każdego $s\in\mathcal{S}$ zachodzi $$v_{\pi_{1}}(s)\leq v_{\pi_{1}}(s)$$
Powiemy, że $\pi_{*}$ jest optymalna, jeżeli dla każdej strategii $\pi$ zachodzi   $\pi\prec\pi_{*}$ Nietrudno przy tym zauważyć, że jeżeli $\pi_{*}$ jest optymalną strategią, to spełnione będą równiania:
$$v_{\pi_{*}}(s) =\max_{\pi} v_{\pi}(s)$$ $$q_{\pi_{*}}(a,s) = \max_{\pi} q_{\pi}(a,s)$$

Dopiero teraz przechodzimy do pytania jak znaleźć optymalną strategię? Dzięki temu, że zbiory $\mathcal{S}, \mathcal{A}, \mathcal{R}$ są skończone, to znajomość funkcji $p$ daje nam  ,,pełną wiedzę" o systemie. W szczególności można dokładnie wyliczyć optymalną strategię, bazując na tzw. równiach Bellman'a, które wiążą $v_{\pi}(s)$ z kolejnym stanem systemu $v_{\pi}(s')$
$$v_{\pi}(s) = \sum_{a}\pi(a|s)\sum_{s',r}p(s,a,s',r)[\gamma +\gamma v_{\pi}(s')] $$

Rozsupłanie powyższej ,,rekurencji'' i znalezienie optymalnej strategii  można  sprowadzić  do rozwiązania $|\mathcal{S}|$  równań liniowych i policzenia wartości $v_{\pi}(s)$  dla każdego stanu $s$ dla arbitralnie wybranej deterministycznej (nie losowej )strategii $\pi$. Następnie można ,,poprawiać'' $\pi$ poprzez sprawdzanie, czy dla poszczególnych stanów $s$ i $a$ (takich, że $\pi(s)\neq a$ )będzie $q_{\pi}(s,a)\geq v_{\pi}(s)$. Jeżeli znajdziemy ,,lepszą'' akcję $a$  dla stanu $s$ aniżeli $\pi(s)$, to mamy poprawioną strategię 
$$\pi'(s) =
	\begin{cases}
		a\text{ ,  }s=a,\\			
		\pi(s)\text{ , } s\neq a\\
	\end{cases}
$$
 Okazuje się, że takie poprawianie strategii $\pi\rightarrow\pi'\rightarrow\pi''\rightarrow ...$ prowadzi do optymalnej strategii $\pi_{*}$. Takie podejście w \cite{RL} nazywane jest programowaniem dynamicznym (z ang. \textit{dynamic programming}). Jest to skuteczne podejście jeżeli mamy pełną wiedzę o funkcji $p$ Najczęściej tak jest, jeżeli modeluje się jakieś rzeczywisty system za pomocą procesu decyzyjnego Markowa.\\

Nas jednak bardziej interesuje sytuacja, w której nie znamy funkcji $p$ i będziemy chcieli stworzyć algorytm, które znajduje optymalną strategię doświadczalnie, czyli algorytm, który będzie uczył się systemu, bez uprzednich założeń odnośnie funkcji $p$. Najczęściej zaczyna się od losowej strategii i z każdym cyklem interakcji z systemem (albo po $n$ cyklach) dostosowujemy strategię na podstawie dotychczasowych obserwacji. Ponieważ $v_{\pi}(s)$ jest definiowana jako wartość oczekiwana, to naturalnym odruchem może być wyliczanie przyszłych nagród startując ze stanu $s$ i podążając za strategią $\pi$ w wielu epizodach iterakcji agenta z system, a następnie uśrednienia otrzymanych wartości. Takie podejście, typowo w stylu monte-carlo, ma poważną wadę w przypadku systemów z dużą ilością stanów, bo potrzeba czasami nierealistycznie dużej próbki danych (epizodów), żeby sensownie policzyć  $v_{\pi}(s)$. \\
 
 Metodą, poniekąd łączącą, programowanie dynamiczne z metodami w stylu monte carlo nie wymagających znajomości funkcji $p$ są metody Q-learning (albo ogólnej z ang. \textit{temporal difference}).  Idea tych metod sprowadza się do tego, że możemy w każdy kroku, po każdej interakcji agenta z systemem, poprawiać stosowaną przez niego strategię $\pi$ zgodnie ze wzorem
\begin{equation}
   v_{\pi}(s_{t}) := v_{\pi}(s_{t}) +\alpha [R_{t+1} + \gamma v_{\pi}(s_{t+1}) - v_{\pi}(s_{t+1})]
 \end{equation}
 W tym wypadku nie potrzebujemy czekać do końca epizodu (np. do końca gry), a najlepiej wielokrotnie, żeby zaktualizować $v_{\pi}(s_{t})$ jak w metodach monte carlo. Możemy bazować tylko na otrzymanej nagrodzie i estymacji $v_{\pi}(s_{t+1})$ dla kolejnego stanu. Tutaj $\alpha$ jest hiperparametrem algorytmu, często nazywany z angielskiego \textit{learning rate}. 
 
\section{Opis algorytmu}
Strategię zdefiniowaliśmy jako mapowanie, które każdemu stanowi $s\in\mathcal{S}$ przypisuje rozkład prawdopodobieństwa $\mathbf{P}_{s}$ na przestrzeni stanów $\mathcal{A}$. Teraz jednak chcemy przejść od ogólnej teorii do konkretnego algorytmu grającego w kółko i krzyżyk i w tym celu dokonamy kilku uproszczeń. W szczególności ograniczmy się do strategii nie losowych, tzn, do mapowania
$$\pi : \mathcal{S}\rightarrow\mathcal{A}$$
Jest to szczególny przypadek opisanej w poprzedniej sekcji teorii, gdzie rozważamy wyłącznie atomowe rozkłady prawdopodobieństwa na $\mathcal{A}$. Mając danych stan $s\in\mathcal{S}$ chcemy znać optymalny ruch $\pi(s)\in\mathcal{A}$.  Przy takim uproszczeniu:
	$$\pi_{*}(s) = \argmax_{a\in\mathcal{A}} q(a,s)$$
		oraz
	$$v_{\pi_{*}}(s)=\max_{a\in\mathcal{A}} q(a,s)$$
Optymalna strategia, przy założeniu że znaleźliśmy się w stanie $s$ będzie maksymalizować oczekiwaną nagród wartość $q(a,s)$. Teraz chcielibyśmy włożyć te uproszczenia do (2.1) i na tej podstawie wypracować algorytm nadający się do implementacji. Na początek możemy położyć dla każdego stanu $s\in\mathcal{S}$ i dla każdej akcji $a$, $q(a,s) = 0$.  Dalej załóżmy, że wykonujemy pierwszy cykl interakcji  z systemem. W przypadku gry w kółko i krzyżyk nasz oponent wykonał ruch i gra jest w stanie $s$. Teraz kolej na agenta, który dąży do tego, żeby nauczyć się optymalnej strategii. Zgodnie z (2.2) agent powinien rozważyć wszystkie możliwe ruchy $a$ i wybrać ten, który maksymalizuje $q(a, s)$. Jeżeli jest więcej niż jedno takie $a$, to gracz wybiera je losowo. W każdym razie gracz wybiera  $\tilde{a}$, które maksymalizuje wszytskie $q(a,s)$. System znowu odpowiada na ruch $\tilde{a}$  i przechodzi do stanu $\tilde{s}$ i zwraca nagrodę $R$. Teraz podstawiając $q(\tilde{a},s)$ za $v_{\pi}(s)$ do (2.1) otrzymujemy przypis jak je  zmienić:\\
\begin{equation}
\label{q}
	q(\tilde{a},s) := q(\tilde{a},s) +\alpha[R + \gamma\cdot\max_{a\in\mathcal{A}}q(a,\tilde{s})]
\end{equation}
Możemy zainicjować wszystkie $q(a,s):=0$, a następnie podczas wielokrotnie rozgrywanych gier z systemem aktualizować te wartości po każdym ruchu zgodnie z (\ref{q}). Okazuje się, że takie aktualizacje, albo poprawianie wartości $q(a,s)$ po każdym ruchu, zapewni zbieżność do rzeczywistych wartości  $q(a,s)$. Następnie stosując (2.1) otrzymujemy optymalną startegię gry w kółko o krzyżyk.

\section{Implementacja}

Z poprzedniej sekcji wynika, że powinniśmy zainicjować dla wszystkich możliwych stanów gry $s$ i możliwych ruchów $a$ wartości $q(a,s):=0$ i następnie aktualizować. Wypisanie do listy lub tabeli wszystkich możliwych kombinacji $(a,s)$ dla gry w kółko i krzyżyk jest oczywiście możliwe, aczkolwiek trochę kłopotliwe. Zauważmy również, że jeżeli po ruchu agenta  mamy jakiś rozkład kółek i krzyżyków na planszy, to nie ma znaczenia jaki był poprzedni stan i jaka akcja agenta doprowadziła do tegoż rozkładu kółek i krzyżyków. Innymi słowy, dla dowolnych dwóch par $(a', s')$ i $(a'', s'')$  jeżeli zarówno wybór akcji $a'$ w stanie $s'$ jaki i wybór $a''$ w stanie $s''$ prowadzi do takiego samego stanu gry $s$,  to $q(a', s') = q(a'', s'') = q_{s}$. Ta obserwacja skłania do przypisywaniu wartości $q_{s}$ do stanów gry po ruchu agenta. \\

W samej implementacji konieczne jest jeszcze ustalenia wartości nagród $\mathcal{R}$.  Wartości $q_{s}$ będą w procesie ,,uczenia''  algorytmu dążyć do oczekiwanej nagrody agenta, pod warunkiem, że swoim ruchem doprowadził do stanu $s$. Wybór jest poniekąd arbitralny, ale warto zauważyć, że w grze w ,,kółko i krzyżyk" remis jest dobrym wynikiem. Grając z graczem stosującym algorytm minimax można co najwyżej zremisować. Stąd potrzeba rozróżnienia przegranej, remisu i wygranej. Gdyby na przykład przypisać nagrodę 1 za wygraną, -1 za przegraną i 0 za każdy inny stan planszy, to algorytm grając z minimax uczyłby się tylko nie przegrywać. Nie byłby bowiem w stanie odróżnić wygranej od remisów. W tej implementacji przyjąłem wartości nagród równe: 
\begin{enumerate}
	\item{-1 w przypadku przegranej}
	\item{0.75 w przypadku remisu }
	\item{1 w przypadku wygranej}
	\item{0 dla każdego innego stanu gry}
\end{enumerate}

W implementacji wykorzystuję plik tekstowy, przechowujący wartości $q_{s}$, gdzie $q_{s}$, zgodnie z powyższą obserwacją jest równa wszystkim  $q(a',s')$, które prowadzą do stanu gry $s$ po podjęciu akcji $a'$ w stanie $s'$ przez agenta. Poniżej ilustruję kilka pozycji z tego pliku, już po przetrenowaniu algorytmu (około 20 000 gier rozegranych z graczem losowym) z omówieniem jego struktury.


\begin{flushleft}
	\includegraphics [scale=0.8] {QTable_ex.png}
\end{flushleft}

Pierwsze 9 cyfr rozdzielonych przecinkami przedstawia stan gry. Przy czym pierwsza cyfra opisuje pole w lewym górnym rogu planszy. Druga cyfra opisuje środkowe pole w górnym wierszu planszy. Trzecia cyfra pole w prawym górnym rogu. Kolejne trzy cyfry opisują środkowym rząd planszy idąc od lewej do prawej. Ostatnie 3 cyfry, opisują idąc od lewej do prawej dolny rząd planszy.  Cyfry 1 oznaczają ruchy systemu, czyli w tym wypadku człowieka, który gra z algorytmem. Cyfry 2 przedstawiają ruchy agenta, czyli grającego algorytmu. Cyfra 0 oznacza puste pole. Ostatnia liczba rzeczywista oznacza wartość  $q$ dla każdego ruchu agenta, który doprowadzi do stanu planszy reprezentowanego prze pierwsze 9 cyfr. 

Rozważmy pierwszy wiersz z powyżej ilustracji. Dla ustalenia uwagi załóżmy, że system gra ,,kółkiem", a agent ,,krzyżykiem". Ciąg $2, 0, 1, 2, 1, 2,  2, 1, 1.0$ Przedstawia od następujący stan gry po ruchu agenta.

\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_1.png}
\end{flushleft}

To jest sytuacja w której agent wygrywa, stąd wartość $q$ dla tego stanu jest równa 1. \\
Kolejny wpis w tabeli, to ciąg $1, 2, 2, 2, 1, 1, 0, 1, 2, 0.75$. Pierwsze 9 cyfr przedstawiam następujący stan gry:

\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_2.png}
\end{flushleft}
Jak widać, niezależnie od ruchu gracza reprezentującego system (gracz posługujący się kółkiem), gra zakończy się remisem. Stąd wartość $q$ w tym przypadku jest równa $0.75$. \\

Kolejny przykład, to ciąg $2, 2, 1, 2,  2, 1, 1, 0, 0, -1$, jak łatwo zauważyć przedstawiam sytuację, w której  ruch należy do gracza systemowego (kółko lub cyfra 1) i gracz ten może z łatwością wygryać uzyskując trzy ,,kółka'' na prawej skrajnej kolumni. Stąd wartość $q$ w tym przypadku jest równa $-1$.
\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_3.png}
\end{flushleft}

Czwarty przykład odpowiadający ciągowi $2, 0, 0, 1, 2, 2, 0, 0, 0, 0.418681257165$  to gra w stanie jeszcze mało zaawansowanym, w którym obaj gracze mają szansę wygrać lub przegrać. W tym przypadku wyczuona wartość $q$ wynosi około $0.42$, co wskazywałoby, że jest to relatywnie korzystna sytuacja dla agenta grającego ,,krzyżykiem''.
\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_4.png}
\end{flushleft}
Ostatni przykład, to ciąg $ 2, 0, 2, 2, 1, 0, 1, 1, 0, -0.0186631984604$ Wartość $q$ lekko ujemna wskazuje na neutralność tego stanu. Ten przykład jest ciekawy i pokazuje słabość algorytmu. System może w kolejnym ruchu wygrać grę. Pamiętajmy jednak, że algorytm uczył się grając z graczem losowym, który nie koniecznie wykorzysta możliwość wygrania i z tego stanu możliwe jest nadal zwycięstwo agenta (ale remis już nie). Przypuszczalnie dlatego wartość oczekiwana przyszłych nagród jest bliska zero. Gdyby uczyć algorytm poprzez grę tylko i wyłącznie z idealnym graczem, to najprawdopodobniej ten stan (po przetrenowaniu) miałby wartość bliższą $-1$.
\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_5.png}
\end{flushleft}

Mając wszystkie niezbędne elementy możemy przejść do bezpośredniego zdefiniowania algorytmu:



Zaczynając od pustego zbioru wartości $q_{s}$, po 20 000 rozegranych grach z graczem losowym mamy zapisanych 


\begin{enumerate}
	\item{Inicjuj pusty zbiór z wartościami $q_{s}$}
	\item{Ustaw parametry $\alpha$ i $\gamma$ z przedziału $[0, 1]$}
	\item{Graj n razy z systemem w kółko i krzyzyk:}
	\begin{enumerate}
		\item{Jeżeli jest ruch agenta, a plansza jest w stanie $s-1$:}
		\begin{enumerate}
			\item{Dla każdego legalnego ruchu prowadzącego z $s-1$ do $s$  sprawdź wartość $q_{s}$ w zbiorze, a jeżeli nie ma takiego elementu w zbiorze to przyjmij $q_{s}=0$.}
			\item{Wybierz ten ruch $\tilde{s}$, dla którego $q_{\tilde{s}} =\max_{s} q_{s}$. Jeżeli jest kilka takich ruchów, to wybierz jeden losowo.}
			\item{Połóż $q_{s-1} = q_{s} + \alpha(R(\tilde{s}) + \gamma  q_{\tilde{s}} )$ }
			\item{Zapisz nową wartość $q_{s-1}$ w zbiorze. Jeżeli wartość już istniała w zbiorze, to nadpisz.}
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

Popatrzmy teraz na wyniki uczenia. Zaczynam od pustego zbioru $q_{s}$ i rozgrywam po 100 partii z graczem losowym i z graczem stosującym minmax. Dodatkowo po każdej grze ,,opróżniam" zbiór $q_{s}$ tak żeby algorytm na razie niczego nie uczył się. Robię to po to, żeby mieć punkt odniesienia przy sprawdzaniu na ile dobrze algorytm nauczył się gry. W poniżyszych przykładach agent jest zawsze graczem korzystającym z ,,kółka", a system jest reprezentowany przez ,,krzyżyk". \\

W 100 grach rozegranych z graczem losowym otrzymałem wyniki jak poniżej:
\begin{flushleft}
	\includegraphics [scale=0.7]{Qtable_vs_Rnd_untrained.png}
\end{flushleft}
Jak widać agent jest nieznacznie lepszy od systemu, ale praktycznie w granicach losowego wahania.\\

Natomiast 100 gier rozegranych z graczem minimax dało 10 remisów i 90 przegranych agenta:
\begin{flushleft}
	\includegraphics [scale=0.7]{QTable_vs_Minimax_untrained.png}
\end{flushleft}

Po przeuczeniu algorytmu na 20 000 grach z graczem losowym, zbiór $q_{s}$ ma 775 rekordów (tyle różnych stanów nasz agent napotkał), natomiast wyniki dla 100 gier teraz wyglądają już znacznie lepiej:
\begin{flushleft}
	\includegraphics [scale=0.7]{Qtable_vs_Rnd_trained.png}
\end{flushleft}
Z graczem losowym, wytrenowany agent przegrywa tylko w 3\%, ma 76\% wygranych i 21\% remisów. Grając z tak wytrenowanym agentem można odczuć, że gra całkiem dobrze, ale jeszcze czasami potrafi wykonać poważny błąd. \\

\begin{flushleft}
	\includegraphics [scale=0.7]{QTable_vs_Minimax_trained.png}
\end{flushleft}
Nie ma natomiast prawie żadnej poprawy jeżeli chodzi o liczbę remisów (bo wygrać się nie da) z systemem grającym strategią minimax. Nasuwa się oczywiście pytanie, czy przetrenowanie algorytmu na grach z systemem grającym strategią minimax poprawiłoby tą statystykę?. Poniżej wyniki algorytmu po dodatkowym 10 000 grach z systemem grającym strategią minimax:

\begin{flushleft}
	\includegraphics [scale=0.7]{QTable_vs_Minimax_trained_d.png}
\end{flushleft}
A więc trenowanie algorytmu na systemie grającym strategią minimax odniosło skutki.  Warto jeszcze sprawdzić, czy po tym trenowania algorytm nie odnotuje gorszych wyników grając z graczem losowym:
\begin{flushleft}
	\includegraphics [scale=0.7]{QTable_vs_Rnd_trained_d.png}
\end{flushleft}
Jak widać wyniki uległy nawet poprawie. Agent po dodatkowym trenowaniu wygrywa w 84\%, przegrywa w 2\% i w 14\% remisuje. Należy jednak zaznaczyć, że takie trenowanie algorytmu z systemem, który gra zgodnie ze strategią minimax, nie jest zupełnie zgodne z pierwotną ideą, żeby algorytm nauczył się grać w kółko i krzyżyk bez żadnych dodatkowych informacji, w tym bez możliwości uczenia się przez grę z optymalną strategią. \\

Na koniec dodam jeszcze, że po takim podwójnym trenowaniu, czysto subiektywne odczucie jest takie, że algorytm gra w sposób zbliżony do człowieka. Rzadko popełnia oczywiste błędy, a chwila nie uwagi kończy się zazwyczaj przegraną. 

 
\chapter{Sieć neuronowa grająca w kółko i krzyżyk }\label{r:Siec}

\section{Sieci neuronowe}
Metoda opisana w poprzednim rozdziale ma jedną zasadniczą wadę. Dla gier o znacznie większej liczbie możliwych kombinacji plansz, zbiory z wartościami $q_{s}$ stają się ogromne. O ile jeszcze samo zapisanie takiego zbioru, zważywszy na szybki rozwój elektronicznych nośników danych, mógłby być możliwy, to już przeszukiwanie przed każdym ruchem zbioru w celu wyłonienie tego, który maksymalizuje $q_{s}$ jest absolutnie nie wykonalne. Alternatywą do przechowywania wartości $q_{s}$ w tabeli lub pliku, może być wytrenowanie sieci neuronowej w taki sposób, żeby estymowała funkcję $s\rightarrow q_{s}$\\

Się neuronowa składa się z neuronów. Natomiast jeden neuron ma określoną liczbę danych wejściowych $n$, odpowiadającą tej ilości liczbę wag $w_{i}, 1 < i\leq n+1$, (o jeden więcej niż ilość danych wejściowych) przy czym oznaczymy $w_{n+1}=b$ , oraz zadaną funkcję aktywacji $f:\mathbf{R}\rightarrow\mathbf{R}$.  Załóżmy, że dane wejściowe to $x_{1}, x_{2},...,x_{n}$.  Neuron ,,bierze" sumę ważoną tych danych powiększoną o wyraz stały $w_{n+1}$ i zwraca wartość funkcji aktywacji z tej sumy :

%$$f(\sum_{i=1}^{n}w_{i}x_{i} +b)$$
\begin{flushleft}
	\includegraphics [scale=0.2]{neuron.png}
\end{flushleft}


Funkcje aktywacji może być w zasadzie dowolna. Ze względu łatwość ,,uczenia" sieci neuronowej, o czym za chwilę, warto stosować funkcje różniczkowalne (ewentualnie różniczkowalne poza kilkoma punktmai) i monotoniczne. Najczęściej stosuje się takie funkcje jak sigmoid, tanges hiporbeloczny, relu, czy softplus.  Poniżej definicja i wykres funkcji sigmoid - najbardziej klasycznego przykładu funkcji aktywacji.

$$sigmoid(x) = \frac{1}{1-e^{-x}}$$
\begin{flushleft}
	\includegraphics [scale=0.5]{sigmoid.png}
\end{flushleft}


Sieć neuronowa składa się, jak nazwa wskazuje z wielu neuronów, ułożonych w warstwy. Pierwsza warstwa, to warstwa wejściowa danych. Dane z wejściowe są przekazywane neuronom w w drugiej warstwie z neuronami. Każdy neuron ma swój zestaw wag, które służy do utworzenia kombinacji liniowej, z danych wejściowych, po czym neuron zwraca wartości funkcji aktywacji na tej kombinacji liniowej. Wartości zwracane przez neurony w drugiej warstwie są z kolei przekazywane do trzeciej warstwy z neuronami, tak jak dane wejściowe zostały przekazane do drugiej warstwy (pierwszej z neuronami).  Wartości obliczone na ostatniej warstwie z neuronami, to są wartości ostatecznie zwracane przez sieć. 

\begin{flushleft}
	\includegraphics [scale=0.7]{nn_example.png}
\end{flushleft}

Rysunek powyżej przedstawia przykładową sieć, która ma trzy wartości wejściowe, a następnie warstwę 2 neuronów, warstwę 3 neuronów i ostatnią warstwę wyjściową składającą się z jednego neuronu. Wartości na strzałką pokazuję wartości wag neuronu, do którego prowadzi strzałka. Dodatkowo, na niebiesko zaznaczono wartości stałe ($b$) dla poszczególnych neuronów. Przy tworzeniu sieci neuronowej, ważne jest, żeby ilość wag dla każdego neuronu w k-tej warstwie, była równa ilości neuronów (lub danych wejściowych) w k-1-ej warstwie (plus dodatkowo wyraz stały). W przeciwnym razie nie uda się uwzględnić wszystkich wartości z k-1 warstwy w kombinacji liniowej.  \\
 
Podsumowując można powiedzieć, że siecią neuronową, o zadanej strukturze, jest funkcja $\mathbf{\mathcal{N}}$, której argumenty, to zbiór wartości wejściowych $x_{1},...,x_{n}$ oraz zbiór wszystkich wag $\mathbf{W}$ i zbiór wszystkich wyrazów stałych $\mathbf{b}$, która zwraca wektor wartości $y_{1},...,y_{m}$, którego wymiar jest równy ilości neuronów w ostatniej warstwie.

$$\mathcal{N}(x_{1},...,x_{n}, \mathbf{W}, \mathbf{b}) = (y_{1},...,y_{m})$$

Argumenty $\mathbf{W}$  i $\mathbf{b}$ będziemy traktować jak parametry funkcji $\mathbf{\mathcal{N}}$. Chcielibyśmy tak zmieniać te parametry, żeby sieć neuronowa $\mathbf{\mathcal{N}}$ przybliżała nam jakąś rzeczywistą, obserwowalną, ale nieznaną funkcję $\Phi$: 
$$ (x_{1},...,x_{n})\stackrel{\Phi}{\rightarrow}(y_{1},...,y_{m})$$

Skoro chcemy przybliżać funkcję $\Phi$ siecią neuronową $\mathbf{\mathcal{N}}$, to potrzebna jest jakaś miara skuteczności takiego przybliżenia. Poniższe zarys dotyczący tego jak trenować sieci neuronowa zaczerpnięty został z \cite{nn}. Załóżmy, że mam k-elementowy zbiór danych $\mathbf{X}$ i dla każdego $x_{1},...,x_{n}\in\mathbf{X}$ obserwujemy $\Phi(x_{1},...,x_{n})$ oraz $\mathbf{\mathcal{N}}(x_{1},...,x_{n}, \mathbf{W}, \mathbf{b})$. Jak w tym przypadku określić skuteczność przybliżania $\Phi$ za pomocą sieci $\mathbf{W}, \mathbf{b}$ na zbiorze $\mathbf{X}$? W literaturze można spotkać wiele metod, ale chyba najbardziej naturalną jest średnio kwadratowa funkcja straty (kosztu) , którą zapiszemy jako funkcję parametrów $\mathbf{W}$ i $\mathbf{b}$.
 
\begin{equation}
	\label{Cost_function}
	C(\mathbf{W}, \mathbf{b}) = \frac{1}{2k}\sum_{(x_{1},...,x_{n})\in\mathbf{X}}{\norm{\Phi(x_{1},...x_{n}) - \mathbf{\mathcal{N}}(x_{1},...,x_{n}, \mathbf{W}, \mathbf{b} )} } ^{2}
\end{equation}
 
 
Norma występującą w powyższym wyrażeniu, to zwykła odległość euklidesowa w przestrzeni $\mathbf{R^{d}}$. Zauważmy, że jeżeli ustalimy zbiór testowy $\mathbf{X}$ i zbiór obserwacji $\Phi(\mathbf{X})$, to powyższą funkcję kosztu możemy rozpatrywać jako funkcję parametrów sieci neuronowej $\mathbf{\mathcal{N}}$. Przy takim podejściu, znalezienie wag $\mathbf{W}$ i $\mathbf{b}$, dla których sieć $\mathbf{\mathcal{N}}$ będzie najlepiej przybliżać funkcję $\Phi$ sprowadza się do znalezienia minimum funkcji $C(\mathbf{W}, \mathbf{b}):\mathbf{R^{d}}\rightarrow\mathbf{R}$, dla pewnej liczby naturalnej d. Jeżeli liczba d jest duża, to analityczne szukanie punktów, w których zerują się wszystkie pochodne cząstkowe byłoby niewykonalne. W praktyce przyjmuje się jednak inne podejście. Dla uproszczenia zapisu przyjmijmy, żę $v:=(\mathbf{W}, \mathbf{b}$. Czyli funkcja kosztu C będzie funkcją zmiennej $v\in\mathbf{R^{d}}$. Z rachunku różniczkowego wiadomo, że:
 
\begin{equation}
	\label{nn_u}
	\Delta C = \nabla C \cdot \Delta v
\end{equation}
 
W powyższym równaniu symbol $(\cdot)$ oznacza iloczyn skalarny wektorów. Natomiast $\nabla C$ to jest gradient funkcji C, czyli $\nabla C = (\frac{\delta C}{\delta v_{1}},...,\frac{\delta C}{\delta v_{d}} )$. Zwróćmy uwagę, że $\Delta C$ jest zwykłym skalarem. Jeżeli ustalimy sobie $v$ i pewną małą wartość $\eta>0$, to możemy położyć:
$$v' = v + \eta\nabla C(v), $$
$$ \Delta v = v - v'$$
 
i wstawimy do (\ref{nn_u}), to otrzymamy:
$$\Delta C = \nabla C\cdot (v - v)' = -\eta \nabla C\cdot\nabla C = -\eta\norm{\nabla C}^{2} < 0$$
 
Jeżeli więc będziemy zmieniać komplet wag zgodnie ze wzorem:
$$v':=v+\eta\nabla C(v),$$
to będziemy zmniejszać wartość $\Delta C$, co oznacza, że będziemy podążali w kierunku minimum funkcji C. Ta metoda znana jest w literaturze (np. \cite{nn}) jako ,,\textit{gradient descent}''. Ważnym parametrem w tej metodzie jest $\eta$, często nazywana w angielskojęzycznej literaturze \textit{learning rate}. Jeżeli $\eta$ jest duże, to potrzeba będzie mniej kroków na odnalezienie lokalnego minimum, ale zagrożenie jest takie, że zmieniając wagi będziemy ,,przeskakiwać'' nad minimum niewiele zbliżając się do niego. Z drugiej strony, zbyt małe $\eta$ może skutkować koniecznością wykonania bardzo wielu kroków w celu odnalezienia minimum. Odmianą tej metody jest ,,\textit{stochastic gradient descent}'' (dalej SGD), która dodatkowo różni się tym, funkcję kosztu $C$ (\ref{Cost_function}) wyliczamy na podstawie losowo wybranego podzbioru ze zbioru testowego $\mathbf{X}$ i odpowiadających obserwacji $\Phi(\mathbf{X})$. Dla jednego zbioru obserwacji $\mathbf{X}$ wielokrotnie losować podzbioru, obliczać $\nabla C$ zdefiniowanego dla wylosowanego podzbioru i aktualizować wagi $v$. W literaturze można przeczytać, że taka metoda pozwala na skuteczne dobranie wag. Skrajny przypadek metody STD, to branie tylko jednej obserwacji do wyliczenia funkcji kosztu.\\
 
Reasumując, jeżeli potrafilibyśmy policzyć gradient $\nabla C$ dla funkcji kosztu, to mamy przepis na to jak zmieniać wagi $\mathbf{W}$ i wartosci stałe $\mathbf{b}$, czyli parametry sieci $\mathcal{N}$, w sposób, który gwarantuje nam zmniejszanie nam wartości funkcji kosztu, a więc poprawiający przybliżenie. Tutaj z pomocą przychodzi algorytm propagacji wstecznej (z ang. \textit{,,backpropagation algorithm ``}. Algorytm propagacji wstecznej, jest podstawowym algorytmem, który w połączeniu z powyższymi rozważaniami o tym jak zmieniać wagi sieci, pozwala na skuteczne trenowania sieci. W wielkim skrócie, algorytm propagacji wstecznej sprawdza się do wliczenia wartości aktywacji na każdym neuronie, a następnie - idąc od ostatniej warstwy wyjściowej aż d pierwszej warstwy (stąd zapewne nazwa propagacji wstecznej) obliczenia błędów sieci i modyfikowanie wag na każdym poziomie. Dokładny opis algorytm, jak również dowód poprawności algorytmu, jest dostępny w niemal każdym opracowaniu traktującym o podstawach sieci neuronowych. Ponieważ niniejsza praca raczej ma się koncentrować na praktycznych aspektach uczenia sieci, to opis algorytmu zostanie pominięty.

\section{Podejście I - nieudane}

Biorąc pod uwagę, że trenowanie algorytmu opisanego w poprzednim rozdziale dało całkiem dobre rezultaty, to naturalnym pomysłem było zastąpienie tabeli przypisujących stanom $s$ wartości $q_{s}$ siecią neuronową i pozostawienie pozostałych elementów niezmienionych. W rezultacie mamy taki algorytm:

\begin{enumerate}
	\item{Inicjuj sieć neuronową, która przyjmuje na wejściu 9 wartości i zwraca na wyjściu 1 wartość -  $q_{s}$}
	\item{Ustaw parametry $\alpha$ i $\gamma$ z przedziału $[0, 1]$}
	\item{Graj n razy z systemem w kółko i krzyzyk:}
	\begin{enumerate}
		\item{Jeżeli jest ruch agenta, a plansza jest w stanie $s-1$:}
		\begin{enumerate}
			\item{Dla każdego legalnego ruchu prowadzącego z $s-1$ do $s$  sprawdź zwracaną przez sieć neruonową wartość $q_{s}$}
			\item{Wybierz ten ruch $\tilde{s}$, dla którego $q_{\tilde{s}} =\max_{s} q_{s}$. Jeżeli jest kilka takich ruchów, to wybierz jeden losowo.}
			\item{Połóż $q_{s-1} = q_{s} + \alpha(R(\tilde{s}) + \gamma  q_{\tilde{s}} )$, gdzie $R(\tilde{s})$ jest nagrodą po ruchu $\tilde{s}$. }
			\item{Przetrenuj sieć na jednoelementowym zbiorze \{$s-1$, $q_{s-1}$\} }.
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

Pozostaje jeszcze kwestia ustalenia topologii sieci i tego jaka będzie funkcja aktywacji. Kółko i krzyżyk nie jest skomplikowaną grą, stąd decyzja o sieci z jedną warstwą ukrytą. Warstwa wejściowa, to oczywiście 9 pozycji odpowiadających dziewięciu polom na planszy. Warstwa wyjściowa, to jedna pozycja zwracającą $q_{s}$ dla stanu reprezentowanego przez dane wejściowe. Przy czym podobnie jak w poprzednim rozdziale, dane wejściowe reprezentują stan gry po ruchu agenta. Ilość neuronów w warstwie ukrytej wybrałem poniekąd arbitralnie na $27$. Poniekąd dlatego, że dla symetrii chciałem, żeby to była wielokrotność ilości danych wejściowych. 

\begin{flushleft}
	\includegraphics [scale=0.5]{nn_1pod.png}
\end{flushleft}


 Na koniec jeszcze pozostaje ustalenie funkcja aktywacji. Ponieważ $q_{s}$ reprezentuje wartość oczekiwaną przyszłych nagród, to ważne jest, żeby funkcja aktywacji miała zbiór wartości obejmujący możliwe wartości przyszłych nagród. W poprzednim rozdziale, przyjąłem takie wartości:

\begin{enumerate}
	\item{-1 w przypadku przegranej}
	\item{0.75 w przypadku remisu }
	\item{1 w przypadku wygranej}
	\item{0 dla każdego innego stanu gry}
\end{enumerate}

Ponieważ wygrana, remis, lub przegrana w grze może się zdarzyć tylko raz, to wartość oczekiwana przyszłych nagród, będzie w zakresie  $[-1,1]$ i  stąd dobrze mieć funkcję aktywacji obejmującą ten przedział. Dobrym kandydatem byłaby funkcja $\tanh$, czyli tangens hiberboliczny, którego zbiór wartości to właśnie przedział $[-1,1]$. Problem jednak jest taki, że jeżeli funkcja aktywacji ,,wypłaszcza" się dla dużych argumentów, to jej pochodna jest bliska zeru, co z kolei przekłada się na to, że algorytm propagacji wstecznej (czyli ten, który liczy gradient funkcji kosztu) szacuje pochodne na bliskie zeru i cały algorytm uczenia sieci neuronowej nie aktualizuje wag $\mathbf{W}, \mathbf{b}$. Chcąc uniknąć takiego efektu ,,saturacji", za funkcję aktywacji przyjąłem $f(x) = 2\tanh(x)$. Dla porządku, podaję jak jak policzyć funkcję tangens hiperboliczny i jak wygląda jej wykres.

\begin{flushleft}
	\includegraphics [scale=0.5]{tanh.png}
\end{flushleft}

\begin{equation}
	\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
\end{equation}

Niestety takie proste zastąpienie tabeli przechowującej wartości $q_{s}$ przez sieć neuronową, która miałaby w podobny sposób, po każdym ruchu poprawić swoje oszacowanie $q_{s}$ nie zdało egzaminu. Po długotrwałym trenowaniu (20 000 gier z graczem losowym) taka sieć potrafi wygrywać z graczem losowym w około 60\%-70\% przypadków, i w subiektywnym odczuciu grać zdecydowanie gorzej od człowieka.  Poniżej wynik 100 rozegranych gier na przetrenowanej sieci:

\begin{flushleft}
	\includegraphics [scale=0.7]{nn_I_tanh.png}
\end{flushleft}

Eksperymentowałem  z większą liczbą neuronów w warstwie ukrytej, jak również z dwiema warstwami ukrytymi, oraz z innymi funkcjami aktywacji, ale zawsze z podobnym, niezadawalającym efektem. Najlepsze wyniki jakie mi się udało uzyskać, to około 75\% wygranych gier z graczem losowym. Wyniki takie uzyskałem używając funkcji aktywacji $softplus$, która definiuje się jako  $f(x) = ln(1+e^{x})$ Ta funkcja aktywacji przyjmuje wartości z przedziału $[0,+\infty]$  (est to wygładzona wersja innej popularnej funkcji aktywacji -  ReLu), stąd konieczna była zmiana wartości nagród. W tym przypadku przyjąłem:

\begin{enumerate}
	\item{0.5 w przypadku przegranej}
	\item{75 w przypadku remisu }
	\item{100 w przypadku wygranej}
	\item{1 dla każdego innego stanu gry}
\end{enumerate}

Poniżej wynik rozegranych 100 gier z graczem losowym po przetrenowaniu na 20 000 gier.

\begin{flushleft}
	\includegraphics [scale=0.7]{nn_I_softplus.png}
\end{flushleft}

Jak widać, sieć gra statystycznie nieco lepiej od gracza losowego, czyli algorytm ,,czegoś" się nauczył, ale przy tak prostej grze jak kółko i krzyżyk można by się spodziewać znacznie lepszych rezutlatów. Dodam jeszcze, że dla wszystkich wypróbowanych kombinacji sieci, po przetrenowaniu, miały procent remisów z graczem minimax na poziomie od 0\% do 5\%.


\section{Podejście II - bardziej udane}

Dlaczego sieci z poprzedniej sekcji nie udało się dobrze nauczyć grać w kółko i krzyżyk? Nie jestem oczywiście odpowiedzieć na to pytanie z całą pewnością, ale stawiam hipotezę, że problem leży w tym, że sieć uczy się po każdym ruchu na jedno-elementowym zbiorze treningowym. Takie rozwiązanie ma zasadniczą wadę. Większość ruchów skutkuje nagrodą równą zero. Dopiero ostatni ruch w grze kończy się inną wartością nagrody. Sieć natomiast modyfikuje wagi po każdym ruchu i używa tych zmodyfikowanych wag do dalszej gry i dalszego uczenia.  To może może powodować, że wielokrotnie modyfikując wagi dla ruchów z zerową nagrodą sieć ,,zapomina" wagi, które pozwalały wygrać. Lepszym podejściem byłoby rozegranie wielu gier (wielu ruchów) i przekazanie sieci do trenowania jednego dużego zbioru danych. W tej części, spróbujemy zastosować takie podejście. Pomysł, na którym się wzoruję  zaczerpnąłem z pracy magisterskiej złożonej na uniwersytecie technicznym w Monachium \cite{TUM}.\\

Chcąc mieć pewność, że sieć nie jest za mała, zmieniłem jej topologię dodając jedną warstwę ukrytą. Ostatecznie sieć ma strukturę typu 9-54-27-1, czyli zwiera dwie warstwy ukryte po 54 i 27 neuronów odpowiednio.  Za funkcję aktywacji przyjmuję tangens hiperboliczny, który przyjmuje wartości z przedziału $[-1,1$. Nagrody, czy odpowiedź systemu, ustalam z uwaga, żeby być daleko od ekstremalnych wartości funkcji aktywacji:

\begin{enumerate}
	\item{-0.8 w przypadku przegranej}
	\item{0.5 w przypadku remisu }
	\item{0.8 w przypadku wygranej}
	\item{0 dla każdego innego stanu gry}
\end{enumerate}

Sam proces uczenia sieci wygląda teraz następująco. Ustalam arbitralnie wielkość zbioru do uczenia (w literaturze odpowiednik pojęcia \textit{batch size}) np. na 2500. Następnie rozgrywam tyle gier, żeby zebrać 2500 obserwacji. Przy czym jedną obserwacją będzie teraz wektor przechowujący stan planszy po ruchu agenta (gracza odpowiadającego sieci neuronowej), wartość $q_{s}$ czyli zwróconą przez sieć przyszłą oczekiwaną wartość nagród  oraz rzeczywistą nagrodę $R$  jaką system zwrócił po swoim ruchu lub po zakończeniu gry, jeżeli ruch agenta był ostatnim ruchem. Dla rozjaśnienia zobaczmy jak to wygląda na przykładzie.\\

Pierwsze 9 pozycji odwzorowuje planszę gry. Zero odpowiada pustemu polu, jedynka to pole zajęte przez system (w tym wypadku gracz losowy), a dwójka to pole zajęte przez agenta.
W naszym przykładzie zaczyna agent, stawia krzyżyk (dwójkę) na środkowym polu. Na 10 pozycji  - wartość 0.642  - widzimy oczekiwana wartość nagrody zwróconą przez sieć. Następnie czekamy na ruch systemu, chyba że to jest ostatni ruch w grze, i dopiero po tym ruchu systemu dopisujemy nagrodę jaką otrzyma agent. W tym wypadku będzie to zero, bo gra jeszcze się nie skończy.

 
\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_6.png}
\end{flushleft}
$\text{Obserwacja po pierwszym ruchu:  }[0, 0, 0, 0, 2, 0, 0, 0, 0, 0.642, 0]$\\

 Następnie system stawia swoje kółko w na pozycji $2,1$, czyli w pierwszej kolumnie i w drugim rzędzie. Tego stanu planszy nie zapisujemy, bo interesują nas tylko stany po ruchu agenta. Warto jednak nadmienić, że po ruchu systemu sprawdzam jaka jest nagroda agenta i tym samym, dopiero po tym ruchu systemu mamy w pełni uzupełnioną pierwszą obserwację. Kolejny, trzeci ruch w grze, należy do agenta i ten wybiera (sprawdzając jaki ruch będzie skutkował maksymalną oczekiwaną nagroda $q_{s}$) pozycję $1,2$:
 
\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_7.png}
\end{flushleft} 
$\text{Obserwacja po drugim ruchu agenta:  }[0, 2, 0, 1, 2, 0, 0, 0, 0, 0.723, 0$\\

System - gracz losowy - w kolejnym ruchu nie blokuje agenta, tylko stawia kółko na pozycji $3,3$. W odpowiedzi na to, agent wybiera $3,2$ i wygrywa grę, stąd nagroda jest teraz równa $0.8$.

\begin{flushleft}
	\includegraphics [scale=0.2] {ttt_7.png}
\end{flushleft} 
$\text{Obserwacja po trzecim i ostatnim ruchu agenta ruchu agenta:  }[0, 2, 0, 1, 2, 0, 0, 2, 1, 0.843, 0.8]$\\

Powyższa gra dała w wyniku trzy obserwacje. Podczas jednej \textit{epoki} rozgrywamy tyle gier, żeby otrzymać 2500 obserwacji (albo tyle na ile ustalimy \textit{batch size}.) Kolejnym krokiem jest zbudowanie na bazie tych obserwacji zbioru referencyjnego, na podstawie którego będziemy uczyć sieć. W tym celu chcemy zastąpić prognozy $q_{s}$ jakie zwracała sieć ,,poprawnymi'' wartościami $\tilde{q}_{s}$ .  Ustalamy parametr $\alpha\in(0,1)$ i postępujemy jak następuje:


\begin{enumerate}
	\item{Jeżeli dana obserwacja jest ostatnią obserwacją w grze, to $\tilde{q}_{s}= R$}
	\item{W przeciwnym przypadku mamy bierzemy $N$ obserwacji pozyskanych z rozpatrywanej gry i w dla $i=N-1, N-2,...,1$: }
	\begin{enumerate}
		\item{$\tilde{q}_{s-i}= (1-\alpha^{i})q_{s-i} + \alpha^{i}q_{s} $}
	\end{enumerate}
\end{enumerate}
W skrajnym przypadku, jeżeli $\alpha=0$, to zmiana $q_{s}$ na $\tilde{q}_{s}$ sprowadzała by się wyłącznie do przypisywania $\tilde{q}_{s}$ wartości nagrody z końca gry. W drugim skrajnym przypadku, jeżeli $\alpha=1$, to wszystkie $q_{s}$ z danej gry przyjmują wartość końcowej nagrody. Pomiędzy tymi skrajnościami modyfikujemy $q_{s}$ w zależności od końcowego wyniku gry.  

Stosując powyższą regułę do ciągu 2500 obserwacji otrzymujemy zbiór referencyjny, który po służy do uczenia sieci. Po przetrenowaniu sieci i aktualizacji wag, całą procedurę powtarzamy przez tyle epok ile chcemy. Poniżej przedstawiam wyniki jakie udało mi się uzyskać po 8 epokach, czyli bazując na 20 000 obserwacji.

\begin{flushleft}
	\includegraphics [scale=0.7] {nn2_1.png}
\end{flushleft} 

Jak widać agentowi na 100 gier udało się wygrać z graczem losowym w 95\% przypadków.  Można uznać, że to jest całkiem niezły wyniki, aczkolwiek w tym wypadku pomogła trochę losowość. Wydaje się, że procent wygranych gier placuje się w okolicach 90\% przypadków. Poniżej przykładowy wynik po 1000 grach. 

\begin{flushleft}
	\includegraphics [scale=0.7] {nn2_2.png}
\end{flushleft} 

A więc teraz agent gra dużo lepiej niż gracz losowy i ma lepsze wyniki niż sieć z I podejścia. Patrząc na wyniki gracza stosującego algorytm z 2 rozdziału (wygrane na poziomie 75\% -85\%)   można by przypuszczać, że sieć sobie radzi co najmniej nie gorzej. Nie jest to jednak poprawny wniosek. Algorytm z 2 rozdziału wygrywa rzadziej, ale też znacznie rzadziej przegrywa (w około 2\%) w stosunku do średnio 5\% przegranych w przypadku gracza bazującego na przetrenowanej sieci. Okazuje się, że ta różnica w ilości przegranych gier ma kluczowe znaczenie. Gracz ,,neuronowy'' prawie zawsze  przegrywa z graczem stosującym algorytm Q-learning z drugiego rozdziału. Oto wyniki dla 100 rozegranych partii pomiędzy dwoma algorytmami:

\begin{flushleft}
	\includegraphics [scale=0.7] {nn2_3.png}
\end{flushleft} 

Warto jeszcze podkreślić, że algorytm z drugiego rozdziału stał się najbardziej efektywny po dodatkowych ,,sparingach"  z graczem  stosującym minimax. W wypadku sieci neuronowej dodatkowe trenowanie na bazie obserwacji uzyskanych podczas gier z graczem stosującym minimax nie przyniosło żadnej poprawy. Algorytm z rozdziału drugiego, przetrenowany dodatkowo na algorytmie minimax, prawie zawsze remisuje z graczem stosującym algorytm minimax, natomiast gracz wykorzystujący sieć neruonową już tylko w okolicach 40\%-50\% przypadków, co ilustruje poniższy wynik rozegranych 100 gier.

\begin{flushleft}
	\includegraphics [scale=0.7] {nn2_4.png}
\end{flushleft} 

Reasumując, sieć nauczyła się skutecznie wygrywać z graczem losowym, ale niekoniecznie dobrze grać w kółko i krzyżyk, stąd pozostaje nieco słodko-gorzki smak po wykorzystaniu sieci neuronowych do gry w kółko i krzyżyk.

Na koniec podjąłem jeszcze jedną próbę wytrenowania sieci bazując na zasadzie modyfikacji $q_{s}$ jak w I podejściu, ale tym razem nie trenując sieci w locie, po każdym ruchu, ale znowu rozgrywając tyle gier ile potrzeba do uzyskania 2500 obserwacji i trenowanie sieci dopiero na pełnym zbiorze obserwacji. Dokładniej,  $q_{s}$ czyli oczekiwaną wartość przyszłych nagród dla planszy w stanie $s$ po ruchu agenta z $s-1$ do $s$, modyfikowałem zgodnie z regułą (zobacz \cite{RL} rozdział 6.5):
$$q_{s} := q_{s} + \alpha(R_{s}+\gamma\max_{s+1}q_{s+1}-q_{s}),$$
gdzie podobnie jak w poprzednich rozdziałach $\alpha$ to parametr sterujący szybkością uczenia się algorytmu (\textit{learning rate}), $\gamma$ to parametr dyskontujący przyszłe nagrody, $R(s)$ to nagroda po odpowiedzi systemu na ruch agenta z $s-1$ do $s$. Niestety, tak jak w poprzednim podejściu, trenując sieci na 20 000 obserwacji (8 epizodów) nie udało się uzyskać wyniki rzędu 70\%  wygranych, 20\% przegranych i 10\% remisów z graczem losowym. A więc wyniki porównywalne z tymi uzyskanymi w podejściu I. Nieznaczna poprawa może wynikać z dodatkowej warstwy ukrytej. 
  

\chapter{Podsumowanie}
Celem pracy było zaimplementowanie i opisanie kilku wybranych algorytmów grających w grę ,,kółko i krzyżyk" i ten cel został osiągnięty,  aczkolwiek w przypadku algorytmu bazującego na sieciach neuronowych, poziom gry pozostawia wiele do życzenia.  W poniższych akapitach opisuję po krótce, każdy z trzech algorytmów wraz z refleksjami odnośnie ich skuteczności,  możliwych ulepszeń w samym algorytmie lub w sposobie implementacji.\\

Minimax jest najbardziej klasycznym podejściem w przypadku gier z dwoma graczami o zerowej sumie nagród,  czyli gier, w których maksymalizacja wygranej jednego z graczy jest równoważna z minimalizacją wygranej oponenta.   Mimo, że kółko i krzyżyk składa się maksymalnie z 9 sekwencji ruchów, a więc drzewo gry ma 9 poziomów, to algorytm wydaje się ciężki i gra zajmuje dosyć długo . Grając z komputerem trudno to odczuć, ale ewidentnie do widać podczas trenowania innych algorytmów poprzez grę z graczem grającym minimax. Np. rozegranie na komputerze autora1000 gier pomiędzy dwoma graczami grającymi losowo zajmuje kilka sekund, a rozegranie 1000 gier pomiędzy graczem grającym losowo z graczem grającym minimax, to już są minuty. Gdyby dodać jeszcze jeden poziom do gry (takie kółko i krzyżyk na planszy 4 razy 4), to oczekiwanie na ruch komputera byłoby nie do zaakceptowania. W takim przypadku trzeba by koniecznie kończyć przeszukiwanie drzewa na jakimś wcześniejszym poziomie, tak jak to robią algorytmy grające w szachy.  Nieznaczną poprawę szybkości działania przyniosło w przycinanie $\alpha-\beta$. W pojedynczej grzez z komputerem nie daje się tego odczuć, ale przy ,,masowym" rozgrywaniu 1000 jest to nieznacznie szybciej. Podobno najlepsze algorytmy grające w szachy przeszukują drzew gry do 12-13 poziomów, więc pewnie poprawiając struktury danych na bardziej optymalne dało by się odciążyć i przyspieszyć tą implementację.\\

Największym pozytywnym zaskoczeniem był algorytm  \textit{Q- table} bazujący na algorytmie \textit{Off-policy Temporal Difference(0)} opisanym w rozdizale 6.5 w \cite{RL}. Zaczynając z pustą tabelą wartości $q_{s}$ już po 5000 gier mamy tabela wypełnia się około 600-700 stanami z odpowiadającymi im $q_{s}$ i algorytm wygrywa z graczem losowym w ponad 70\% przypadków. Trenowanie algorytmu na 20 000 grach skutkuje graczem na zbliżonym do człowieka. W zakresie implementacji kluczową rzeczą było, żeby nie otwierać i nie zamykać pliku z  tabelą  ze stanami i wartościami $q_{s}$ ponieważ jest to czasochłonne i opóźnia działanie programu. Czy to przy  jednorazowej grze (człowieka z graczem posługującym się algorytmem Q-table), czy przy trenowaniu algorytmu podczas wielu grach,  plik jest otwierany na początku, następnie przez całą sesję przetrzymywany w pamięci jest słwonik Q-table, i na końcu jest zapisywany do pliku.\\ 

Po sukcesach z algorytmem Q-table, wydawało się, że łatwo uda się wytrenować prostą sieć neuronową, która zamiast posługiwać się danymi tabelarycznymi (zapisywanymi do pliku), skompresuje tą informację w macierzach z wagami sieci. Niestety rzeczywistość nie potwierdziła tych optymistycznych założeń i trenując sieć na różne sposoby miałem wrażenie dosięgania,,szklanego sufitu", którego nie udało się przebić. Istotnym jest pytania, co jeszcze można by potencjalnie zrobić, że nauczyć sieć dobrze grać w kółko i krzyżyk.  Na myśl przychodzi więcej warstw ukrytych oraz inne sieciowo-neuronowe metody typu    Wydaje się jednak, że to idzie w kierunku strzelania z armaty do muchy. Sieć z jedną warstwą ukrytą jest w stanie całkiem dobrze rozpoznawać odręcznie pisane cyfry \cite{nn}. Trudno przypuszczać, żeby bądź co bądź banalna, gra w kółko i krzyżyk wymagała tak zaawansowanych metod. Może to kwestia dobrania odpowiednich hiper parametrów? Chociaż nie próbowałem stosować żadnych formalnych metod przeszukiwania przestrzeni hiper parametrów, to raczej wątpię, żeby ich dobór mógł zaowocować prawdziwym przełomem.  W internecie można znaleść kilka ciekawych podejść, jednak większość  nie jest satysfakcjonujących. Jednym z takich podejść jest klasyfikowanie ruchów (dobry lub zły) za pomocą algorytmu minimax. Jeżeli gracz grający algorytmem minimax wybrałby ten ruch, to jest to dobry ruch, a w przeciwnym razie uznajemy ruch za zły. Następnie generując $n$ ruchów podczas gry z graczem losowym, możemy łatwo zbudować zbiór do trenowania sieci. Inne osoby z powodzeniem trenowały sieć korzystając z wcześniej ,,wytrenowanej" tabeli Q-table.  Takie drogi zupełnie mnie nie interesowała, jako że ogólnie wiadomo, że sieć można dobrze wytrenować (dopasaować o danych). Znacznie bardziej ciekawym pytaniem jest jak wykorzystać sieć neuronową do nauczenia się  jak grać w kółko i krzyżyk bez wspierania się już istniejącymi algorytmami; bez żadnej a priori wiedzy, poza samymi regułami gry. Stąd mój nacisk na trenowanie tylko i wyłącznie z graczem losowym, co w rezultacie dało całkiem niezłe wyniki przeciwko takiem graczu (ponad 90\%), co jednak nie przełożyło się na umiejętność grania w potocznym, ludzkim, sensie. \\

Pierwotnie zamierzałem zająć się jeszcze jednym podejściem, a mianowicie algorytmem genetycznym. Zasadę działania takiego algorytmu można łatwo zobrazować. Na początku potrzebny jest relatywnie duży zbiór strategii $\pi_{i}$ indeksowany jakimś parametrem $i$. Następnie, losujemy pary strategii, której grają między sobą w kółko i krzyżyk. Pokonana strategia znika (ginie), a zwycięska może się reprodukować, ale potomstwo $\pi_{\tilde{i}}$ mutuje z pewnym prawdopodobieństwem. W efekcie otrzymujemy nowy zbiór strategii zawierający zwycięską strategię $\pi_{i}$ oraz jej potomstwo $\pi_{\tilde{i}}$.  Sekwencje rywalizacji a następnie reprodukcji z możliwością mutacji powtarzamy wielokrotnie, najlepiej aż do pozostania jednej, ewolucyjnie najlepszej strategii. Jak widać opis wydaje się prosty i zachęcający. Niestety trudności związane z wymyśleniem jak w ogóle charakteryzować strategie gry w kółko i krzyżyk, albo jak zapisać kod DNA danej strategii za pomocą parametrów będących obiektami matematycznymi, przerósł możliwości autora. 

\appendix

\chapter{Kody}
W tej części opisuję podstawowe informacje dotyczące kodów napisanych w ramach niniejszej pracy oraz modułów (bibliotek) innych autorów, których korzystałem.  Nie opisuję natomiast poszczególnych elementów kodu i ich działania. Sam kod źródłowy, napisany w języku Python w wersji 2.17, stanowi załącznik do niniejszej pracy i mam nadzieję, że można się w nim z grubsza zorientować czytając komentarze. Całość kodu składa się z kilku plików:
\begin{itemize}
	\item{\textbf{ttt.py} - zawiera implementację głównego programu i klas graczy}
	\item{\textbf{graphics.py} - moduł obsługujący grafikę autorstwa Johna Zole}
	\item{\textbf{neural.py} - moduł z implementacją sieci neuronowej w narzędziu \textit{Tensorflow}}
	\item{\textbf{QTable.txt} - plik tekstowy z wyuczonymi danymi dla algorytmu Q-Table}
	\item{ \textbf{ttt\_model.index, ttt\_model.meta, ttt\_model.data-000000-of-000001} - dane z wagami sieci neruonowej}
\end{itemize}

\section{Główny program}
Jądrem programu jest funkcja \textit{main()}, która jest odpowiedzialna za przeprowadzenie pojedynczej gry w kółko i krzyżyk. Funkcja przygotowuje planszę i zaznacza graficznie ruchy poszczególnych graczy, chyba że podając odpowiedni parametr wyłączymy grafikę, co jest niezbędne w przypadku rozgrywania tysięcy gier przy uczeniu algorytmów. Decyduje na podstawie podanych parametrów (domyślnie losowo) o tym, do którego gracza należy pierwszy ruch i wymaga wskazania dwóch graczy, przy czym mogą to być gracze stosujący różne algorytmy lub ludzie (w przypadku wybrania opcji włączonej grafiki). Funkcja \textit{main()} korzysta też z kilku funkcji pomocniczych, które sprawdzają, na przykład, czy po danym ruchu jest remis, czy gra się zakończyła wygraną  lub obsługują aktualizację grafiki. Program korzysta z elementarnych modułów \textit{random, sys, copy} oraz ze znanego modułu \textit{numpy} ułatwiającego obliczenia numeryczne, zwłaszcza w przypadku wielowymiarowych struktur danych, takich ja (n-wymiarowe)macierze.  

\section{Klasy graczy}
Opisana wyżej funkcja \textit{main()} wymaga przekazania jako argumentów dwóch instancji obiektów typu \textit{Player}, czyli obiektów reprezentujących graczy. Ogólnie, klasa typu \textit{Player} ma tylko jeden argument - symbol, który wskazuje czy gracz będzie grał kółkiem czy krzyżykiem. Natomiast każda ,,właściwy" typ gracza jest definiowany w osobnej podklasie:
\begin{enumerate}
	\item{Człowiek: klasa \textit{playerHuman}}
	\item{Gracz grający losowo: klasa \textit{playerRandom}}	
	\item{Cracz grający algorytmem minimax: klasa \textit{playerMinimax}}
	\item{Gracz grający algorytmem Q-Table: klasa \textit{playerQTable}}
	\item{Gracz grający z użyciem sieci neuronowej: klasa \textit{playerNeural}}
\end{enumerate}
Każda z tych podklas klasy \textit{Player} ma jedną główną metodę \textit{move()}, która przyjmuje stan  gry jako argument i wybiera najlepszy według siebie ruch. W przypadku klasy \textit{playerHuman} metoda \textit{move()} śledzi pozycję kursora i sprawdza gdzie na  planszy nastąpiło kliknięcie.  \\

Gdyby była potrzeba dodania kolejnej wersji algorytmu grającego w kółko i krzyżyk, np. algorytmu genetycznego, to poza dodaniem odpowiedniej opcji w GUI gry, ,,wystarczyłoby" dodać nową podklasę \textit{playerGenetic} ze stosowną metodą \textit{move()}, która przyjmuje głównie 9 elementowy ciąg 0,1 i 2 obrazujący sytujację na planszy i zwraca cyfrę od 1 do 9, która oznacza wybrane pole do ruchu.  

\section{Grafika}
Istnieje wiele modułów pozwalających tworzenie obiektów graficznych w Pythonie. Jako, że gra w kółko i krzyżyk nie jest graficznie wymagająca, to skorzystałem z bodajże najprostszego narzędzia napisanego przez Johna Zello. Podobno ten moduł został stworzony nie tyle z myślą o poważnych zastosowaniach graficznych, ale z myślą żeby uczyć programowania obiektowego na bazie prostych graficzyncyh obiektów typu punkt, linia, koło itp.  Ogólnodostępny kod źródłowy tego modułu graficznego można znaleźć tutaj.

\section{Tensorflow}
Moim pierwotnym zamysłem była implementacja  sieci neuronowej od początku do końca za pomocą podstawowych modułów python'a. I chociaż miałem na tym polu pewne sukcesy, to jednak zwyciężył pragmatyzm i ostatecznie sieć neuronowa opisana w rozdziale 3, w sekcji 3.3 powstała z wykorzystaniem bibliotek \textit{Tensorflow}. Jest to narzędzie stworzone przez potentata Google, i służy ogólnie do budowania grafów, w których węzłach mają miejsce obliczenia (w tym różnego typu optymalizacje), natomiast krawędzie ilustrują przepływ danych pomiędzy obliczeniami. Praca z tym narzędzie polega na zbudowaniu lub wczytaniu grafu, następnie zapewnienie niezbędnych danych wejściowych, przeprowadzeniu obliczeń po kolei zgodnie ze strukturą grafu i odczytaniu wyniku.  Więcej o o module \textit{Tensorflow} oraz o dostępnym w python'ie API  \textit{Tensorflow}do  można się dowiedzieć na oficjalnej stronie tutaj.


\chapter{Aplikacja}

Niezależnie od statystycznych wyników, jakie poszczególne algorytmy osiągają grając w kółko i krzyżyk, ciekawie jest samemu przekonać sie o 
skuteczności algorytmu grając z nim	,,osobiście". Taka ciekawość była główną motywacją do przygotowania w ramach niniejszej pracy 
bardzo prostej aplikacji gry w kółko o krzyżyk, która zostałą opisana w tym dodatku. \\

Aplikacja ma tylko jeden ekran (zobacz poniższy rysunku), przedstawiający planszę do gry w kółko i krzyżyk. Po uruchomieniu aplikacji 
należy za pomocą niebieskich przycisków umieszczonych na dole planszy wybrać sobie przeciwnika. Można do wyboru grać z drugą osobą (człowiekiem), 
z graczem grającym w sposób losowy,  graczem stosującym algorytm minimax, stosującym algorytm Q-Table lub graczem wykorzystującym sieć neuronową 
z 3 rodziału. Dodatkowo w prawym dolnym rogu aplikacja informuje o tym, którego gracza jest obecnie ruch.

\begin{flushleft}
	\includegraphics [scale=0.5] {plansza.png}
\end{flushleft} 



W celu uruchomienia aplikacji trzeba mieć na komputerze zainstalowanego Pythona 2. Dodatkowo należy w jednym wybranym katalogu umieścić następujące 
pliki:
\begin{enumerate}
	\item{ttt.py}
	\item{graphics.py}
	\item{neural.py}
	\item{Qtable.txt}
	\item{ttt\_model.meta}
	\item{ttt\_model.index}
	\item{ttt\_model.data-00000-of-00001}
\end{enumerate}
Pliki 4 - 7 przechowują parametry wytrenowanych algorytmów Qtable i sieci neuronowej. Jeżeli tych plików zabraknie, to aplikacja będzie działa, ale 
gracze stosujący te algorytmy będą grali tak jak gracz losowy. Samo uruchomienie aplikacji odbywa się, przez otworzenie pliku ttt.py za pomocą programu Python. 


\begin{flushleft}
	\includegraphics [scale=0.5] {uruchamianie.png}
\end{flushleft} 


Moduł ttt.py może również służyć do trenowania algorytmów QTable i sieci neuronowej, ale do tego nie ma GUI.

\begin{thebibliography}{99}

\addcontentsline{toc}{chapter}{Bibliografia}

	\bibitem {nn} Michael Nielsen, \textit{Neural Networks and Deep Learning}, Determination Press, 2015

	\bibitem {itsl} James Gareth, \textit{Introduction to Statistical Learning}, Springer, 2013
	
	\bibitem {RL} Richard S. Sutton and Andrew G. Barto, \textit{Reinforcement Learning: An Introduction},  MIT Press, Cambridge, MA, 2018
	
	\bibitem {MIT_AI} Prof. Patrick Henry Winston, \textit{MIT course number 6.043, Artifical Inteligence}, https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/index.htm
	
	\bibitem{Game} Matthew O. Jackson , \texit{A Brief Introduction to the Basics of Game Theory},\linebreak https://www.ethz.ch/content/dam/ethz/special-interest/gess/chair-of-sociology-dam/documents/education/spieltheorie/literatur/Einführung/Jackson\linebreak\%20Basics\%20of\%20Game\%20Theory\%20SSRN-id1968579.pdf
	
	\bibitem{TUM} Michael Heining, \textit{Dynamic Learning: A case study on Tic-Tac-Toe}, https://www-m15.ma.tum.de/foswiki/pub/M15/Allgemeines/PublicationsEN/MasterThesisHeining.pdf

\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
